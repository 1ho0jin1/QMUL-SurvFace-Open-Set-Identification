{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Set Identification codes for QMUL-SurvFace\n",
    "\n",
    "- This .ipnyb file must be located in the root directory as below:\n",
    "```\n",
    "root\n",
    "|- Face_Identification_Evaluation\n",
    "|- Face_Identification_Test_Set\n",
    "|- Face_Verification_Evaluation\n",
    "|- Face_Verification_Test_Set\n",
    "|- training_set\n",
    "|- readme.txt\n",
    "|- THIS_FILE.ipnyb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMUL_trainset(Dataset):\n",
    "    def __init__(self, transform, img_size=112):\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.img_files = []\n",
    "        self.labels = []\n",
    "        self.class_dict = {}\n",
    "        \n",
    "        train_dir = \"training_set\"\n",
    "        name_list = sorted(os.listdir(train_dir))\n",
    "        ID = 0\n",
    "        for name in name_list:\n",
    "            name_dir = os.path.join(train_dir, name)\n",
    "            img_list = os.listdir(name_dir)\n",
    "            for img_name in img_list:\n",
    "                img_dir = os.path.join(name_dir, img_name)\n",
    "                self.img_files.append(img_dir)\n",
    "                self.labels.append(ID)\n",
    "            ID += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = Image.open(self.img_files[idx]).resize((self.img_size,self.img_size))\n",
    "        img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class QMUL_evalset(Dataset):\n",
    "    def __init__(self, set_arg, transform, img_size=112):\n",
    "        self.set_arg = set_arg\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.img_files = []\n",
    "        self.labels = []\n",
    "        self.class_dict = {}\n",
    "        \n",
    "        if set_arg == \"U\":\n",
    "            base = \"Face_Identification_Test_Set/unmated_probe\"\n",
    "            imgs = os.listdir(base)\n",
    "            self.labels = [-1]*len(imgs)\n",
    "            for img in imgs:\n",
    "                self.img_files.append(os.path.join(base,img))\n",
    "        else:\n",
    "            if set_arg == \"G\":\n",
    "                base = \"Face_Identification_Test_Set/gallery\"\n",
    "                meta = loadmat(\"Face_Identification_Test_Set/gallery_img_ID_pairs.mat\")\n",
    "                imgs = meta[\"gallery_set\"].reshape(-1)\n",
    "                labels = (meta[\"gallery_ids\"]-1).reshape(-1).tolist()\n",
    "            elif set_arg == \"K\":\n",
    "                base = \"Face_Identification_Test_Set/mated_probe\"\n",
    "                meta = loadmat(\"Face_Identification_Test_Set/mated_probe_img_ID_pairs.mat\")\n",
    "                imgs = meta[\"mated_probe_set\"].reshape(-1)\n",
    "                labels = (meta[\"mated_probe_ids\"]-1).reshape(-1).tolist()\n",
    "            ID = -1\n",
    "            for img,lab in zip(imgs,labels):\n",
    "                if lab not in self.class_dict.keys():\n",
    "                    ID += 1\n",
    "                    self.class_dict[lab] = ID\n",
    "                self.img_files.append(os.path.join(base,img[0]))\n",
    "                self.labels.append(ID)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = Image.open(self.img_files[idx]).resize((self.img_size,self.img_size))\n",
    "        img = self.transform(img)\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "def get_lr(optimizer):\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "    \n",
    "\n",
    "# fetch DIR at specific FAR level\n",
    "def dir_at_far(dir_tensor,far):\n",
    "    idx = torch.argmin(torch.abs(dir_tensor[:,2]-far))\n",
    "    return dir_tensor[idx,1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic transformation for evaluation\n",
    "# assuming a float image with values 0.0 ~ 1.0\n",
    "trf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.FloatTensor([0.5, 0.5, 0.5]), torch.FloatTensor([0.5, 0.5, 0.5])),\n",
    "])\n",
    "\n",
    "Gset = QMUL_evalset(\"G\",trf)  # Gallery Set\n",
    "Kset = QMUL_evalset(\"K\",trf)  # Known probe Set (a.k.a. mated probe)\n",
    "Uset = QMUL_evalset(\"U\",trf)  # Unknown probe Set (a.k.a. unmated probe)\n",
    "num_cls = len(set(Gset.labels))\n",
    "print(num_cls)\n",
    "Gloader = DataLoader(Gset,batch_size=200,num_workers=4)\n",
    "Kloader = DataLoader(Kset,batch_size=200,num_workers=4)\n",
    "Uloader = DataLoader(Uset,batch_size=200,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare encoder\n",
    "- This is only an example code\n",
    "- Prepare your own pretrained encoder here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ResNets\n",
    "encoder = ResNets.Resnet(50)\n",
    "pth = torch.load(\"YOUR_DIRECTORY\",map_location=device)\n",
    "encoder.to(device)\n",
    "encoder.load_state_dict(pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    # make Gallery prototypes: averaged features\n",
    "    G_feat = torch.zeros(num_cls, 512).to(device)\n",
    "    cardinality = torch.zeros(num_cls, dtype=torch.int64).to(device)\n",
    "    for batch, (img, label) in enumerate(Gloader):\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        feat = encoder(img)\n",
    "        for i in range(label.size(0)):\n",
    "            G_feat[label[i]] += feat[i]\n",
    "            cardinality[label[i]] += 1\n",
    "    G_feat = torch.div(G_feat.T, cardinality).T\n",
    "\n",
    "    # extract features of known probe set K\n",
    "    for batch, (img, label) in enumerate(Kloader):\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        if batch == 0:\n",
    "            K_feat = encoder(img)\n",
    "            K_label = label\n",
    "        else:\n",
    "            K_feat = torch.cat((K_feat, encoder(img)), dim=0)\n",
    "            K_label = torch.cat((K_label, label), dim=0)\n",
    "\n",
    "    # extract features of unknown probe set U\n",
    "    for batch, (img, label) in enumerate(Uloader):\n",
    "        img = img.to(device)\n",
    "        if batch == 0:\n",
    "            U_feat = encoder(img)\n",
    "        else:\n",
    "            U_feat = torch.cat((U_feat, encoder(img)), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute metrics:\n",
    "TPIR20 @ FAR = [0.01, 0.1, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity\n",
    "# ... or use other metric\n",
    "G_feat = F.normalize(G_feat, dim=1)\n",
    "K_feat = F.normalize(K_feat, dim=1)\n",
    "U_feat = F.normalize(U_feat, dim=1)\n",
    "K_sim = torch.mm(K_feat, G_feat.T)\n",
    "U_sim = torch.mm(U_feat, G_feat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_val, pred = torch.topk(K_sim, k=20, dim=1)  # rank=20\n",
    "U_val, _ = torch.max(U_sim, dim=1)\n",
    "\n",
    "# compute DIR & FAR w.r.t. different thresholds\n",
    "corr_mask = pred.eq(K_label.view(-1,1))\n",
    "DIR_ = torch.zeros(1000, 3)  # intervals: 1000\n",
    "for i, th in enumerate(torch.linspace(min(K_val.min(),U_val.min()), U_val.max(), 1000)):\n",
    "    mask = corr_mask & (K_val > th)\n",
    "    dir_ = mask.sum().item() / K_feat.size(0)\n",
    "    far_ = (U_val > th).sum().item() / U_feat.size(0)\n",
    "    DIR_[i] = torch.FloatTensor([th, dir_, far_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for far in [0.01, 0.1, 0.2, 0.3]:\n",
    "    print(\"TPIR20 @ FAR={}: {:.2f}%\".format(far,dir_at_far(DIR_,far)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
